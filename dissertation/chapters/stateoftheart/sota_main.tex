.% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

\chapter{State of the Art} \label{chapter:sota}

	%\pagenumbering{arabic}
\section{Genetics \textcolor{red}{Change title}}

\subsection{Genetic Variants}
The human genome is composed of around 3 billion nucleotides, them being A, C, T and G, adenine, cytosine, thymine and guanine respectively, with 23 pairs of chromosomes, 1 of them being the sexual chromosomes \cite{zhang2012mining}. It is known that individuals from the same population have similar DNA than to those of different ones \cite{witherspoon2007genetic}. These small differences come in Single Nuclear Polymorphisms, Mutations, Insertions, Deletions and Copy Number Variations, meaning that the variants mentioned are known to be responsible for most of the different phenotypes (observable characteristics) in humans \cite{international2010integrating}. Mutations are extremely rare alterations on the DNA, Indels are, as the name indicates, insertions or deletions that may or may not occur from individual to individual and CNVs are sections of the genome that are repeated, and the number of repeats varies between individuals \cite{international2010integrating}.\\
SNPs are single nucleotide variations that occur in a specific position in the genome, typically with two alleles, and they can either be rare or common. There are around 10 million SNPs in the human genome, meaning that on average a SNP occurs every 300 base pairs  \cite{international2010integrating}. Allele frequencies are given for the less common one, for example, if a SNP can either be a T or a C, if T is the less common one with 0.3 allele frequency, 30\% of the population will have a T, and the rest a C \cite{bush2012genome}. Common variants are the ones with a minor allele frequency of 5\% or more, and rare variants are only present on less than 5\% of the population.\\ %Allele frequencies also help when checking for homozygous and heterozygous alleles,    \textcolor{red}{Talk about homozygous and heterozygous}\\
All of these genetic variants can be used as markers to find associations between genotypes and phenotypes, with the most commonly used for the effect being SNPs, due to their abundance. This association can be fairly straightforward in case of single gene related traits or diseases, but not so much for more complex traits \cite{zhang2012mining}.

\subsection{Type 2 Diabetes and Missing Heritability}
Type 2 Diabetes is one of the most common diseases encountered in the world, and the fifth leading cause of death worldwide. Data from the International Diabetes Federation has shown that, in 2011 there were 366 million people living with diabetes, and that number is expected to rise to 552 million by 2030, 80\% to 90\% of the cases being of T2D \cite{sanghera2012type, prasad2015genetics}. It is also important to note that there is no formal definition for it, since all the cases who do not fulfil the criteria for T1D, LADA and other types, are considered T2D. It's a disease more associated with age, although it has been reported in adolescents \cite{vijan2010type}. The question of how to clinically phenotype T2D is very important, because it can influence its association with genotype, since providing different patterns for the same phenotype will make it harder to perform classification \cite{sanghera2012type}. \\
There is evidence supporting that T2D is strongly influenced by genetic and epigenetic factors, as well as environmental ones. Throughout their lifetime, individuals with one parent who has T2D, have a 40\% risk of developing T2D. This risk increases to 70\% if both of the parents have T2D \cite{ali2013genetics,prasad2015genetics}. Studies performed with twin pairs show a lower discordance rate (one of the twins has the disease, and the other doesn't) for monozygotic twins than for dizygotic, which supports the genetic and epigenetic influence on T2D even further \cite{willemsen2015concordance}. Furthermore, variants associated with T2D in the European population might not be replicated in other non-European populations, and vice-versa. A higher prevalence of the disease is also seen in some populations \cite{sanghera2012type, prasad2015genetics, wang2016genetic}. However, our genetic code doesn't undertake significant alterations in only one or two generations, so this recent surge in predisposition for T2D is also due to the gene-environment interactions. Increase of adipose tissues in human populations is the single most significant factor in this epidemic, and to model the interaction between genes and causes that lead to obesity is extremely complex. Who burns more calories at rest, who has greater exercise levels when not doing it actively, who is more willing to change a sedentary lifestyle are all examples of gene influencing behaviour, and that's what makes the gene-environment interaction so hard to include in Genome Wide Association Studies \cite{ali2013genetics}.\\
So far, for T2D, more than 80 robust markers were found, even though they only account for 20\% of the heritability for this disease. Even more so, these markers are predominantly common, with additive effects \cite{fuchsberger2016genetic}. The hypothesis that a common disease can be caused by several common variants in the genome is not new, and several studies have already identified common alleles who play a role in certain traits or disease susceptibility \cite{yang2010common, fuchsberger2016genetic, bush2012genome, reich2001allelic}. The remaining hypothesis are that a few rare variants have big effects (common disease-rare variant), and that both common and rare variants play a part in susceptibility \cite{prasad2015genetics, sanghera2012type}. Despite the successes of GWAS in identifying markers, much of the heritability in complex diseases still remains unexplained, which leads us to the missing heritability problem.\\
Heritability is defined as a ratio:\[\pi_{explained} = h^2_{known} / h^2_{all} \], where $h^2_{known}$ is the proportion of the phenotype explained by known variants that affect it, and $h^2_{all}$ all the variants, including those who remain undiscovered. The underlying problem is that the $h^2_{all}$ might not be properly estimated, which leads to an underestimation of $\pi_{explained}$. This model also fails to consider epistasis, that can greatly inflate the apparent heritability, and it is not yet consistently detected with the current standard methods available \cite{zuk2012mystery}. It is important to acknowledge the missing heritability problem, because the reasons why this is happening are related to how GWAS are thought-out and performed. The first one is that common variants of low frequency (1-10\%) might not be identified because of the genotyping arrays themselves lacking useful proxies. Secondly, many common variants with very small effects can be extremely hard to identify with current and feasible sample sizes \cite{lander2011initial, huyghe2013exome}. 

\section{Computational Methods for Genome Wide Association Studies}

\subsection{Single and Multi-locus Marker Analysis and Imputation}
After the first mapping of the human genome, on May 27, 2004, the costs of sequencing started to go down along the years, while the number of genome projects published rose \cite{schmutz2004quality}. 
As such, while large and numerous datasets became available, scientists started to develop new ways to analyse and research them. Since 2007, GWAS have been responsible for the discovery of genetic markers that relate to complex human traits and disease, the simplest approach being the Single Marker Analysis, which is capable of finding common variants with addictive effects \cite{scott2007genome}.\\ 
This marker-by-marker analysis focus on the individual effect of each variant, to detect associations between molecular markers and traits or disease in a population, that are also called, Quantitative Trait Loci. This is possible because of Linkage Disequilibrium, that refers to the non-random association of alleles at different loci in a population, and it's visible when their association frequency is higher or lower than what would be expected if it was in fact random \cite{slatkin2008linkage}. It is influenced by many factors, such as rate of mutation, population structure, genetic linkage, the rate of mutation, genetic drift and the system of mating, and can signal segments in the chromosomes that trace back to a common ancestor without intervening recombination. Essentially, the objective is to determine if phenotype differences are due to a few loci with large effects or many loci with small, but additive ones.  \\
We can formalize a contingency table that contains the information of the genotype for each SNP, and the corresponding phenotype for each sample, and use tests such as \textbf{Pearson's $\chi^2$ test}, \textbf{Fisher's Test}, \textbf{G-Test} and so on, to detect an association between allele frequencies and phenotype \cite{zhang2012mining}.
To go even further and detect a QTL with this approach, the association between a marker and a trait, or in this particular case, Type 2 Diabetes, can be modelled by simple regression methods \cite{haley1992simple}. 
The null hypothesis is that the marker has no association with the trait, and to test it, a t-test, F-test or Bayes factor can be used. We assume that a marker will only affect the desired trait if it is in linkage disequilibrium with a QTL. To measure LD, considering A and B as two different markers, A1, A2, and B1, B2 as their alleles respectively:\[D = frequency(A_1\_B_1)\times frequency(A_2\_B_2) - frequency(A_1\_B_2)\times frequency(A_2\_B_1)\], $frequency(A_1\_B_1)$ being the frequency of the haplotype $A_1\_B_1$, and likewise for the remaining haplotypes. Since this is very dependant on allele frequencies, the $r^2$ metric was proposed:\[r^2 = \frac{D^2}{frequency(A_1)\times frequency(A_2)\times frequency(B_1)\times frequency(B_2)}\]\\
By combining $r^2$, LD and the number of markers in the study we can infer the power of the association test to detect QTL's \cite{hill1968linkage,visscher201710}. \\
The following approach tried was the Multi-Locus analysis. It feels like the logical next step, because it tries to tackle issues that the previous method could not, such as consideration of rare variants, epigenetics, and big marker spacing (less genotyped variants) \cite{mooney2012ga}. When building marker-to-marker association models, if our set contains 1 million SNPs, it becomes both statistically and computationally hard to differentiate between significant markers and to understand their biological context, as there will be $5\times10^{11}$ models \cite{turner2011knowledge}. Although a Multi-Locus analysis adds several benefits to it's previous iteration, it  would require even more processing power. At this point, feature selection comes into play, there being several ways to approach it. The most common ones are the usage of SNPs that have met certain criteria in broader previous tests, and integrating biological knowledge in the models. Of course, either one of these strategies imposes bias, such as eliminating potential markers that don't prove to have a significant effect on their own, and missing novel undocumented interactions \cite{carlson2004mapping}. By considering a smaller number of SNPs in which the phenotype information might be contained, it becomes possible to make such an analysis. One example is the Biofilter, that uses previous Biological knowledge to construct several multi-SNPs models, and only then applies Logistic Regression and Multi-factor Dimensionality Reduction methods to perform its analysis \cite{bush2012genome,bush2009biofilter}.\\
To increase the number of SNPs available and generate a common set of genotyped variants for each dataset, for Genome Wide Association Studies, Imputation surfaced as a viable candidate. By using known Linkage Disequilibrium patterns and frequencies of haplotypes from the 1000 Genomes Project, it is possible to make a correct estimate of missing genotypes. However, imputation leads to the underlying assumption that the study population has the same patterns of LD and that the association between haplotypes and causal loci is the same in the reference population, which might not always be the case \cite{10002012integrated, bush2012genome}.

\subsection{\textit{Bayesian} Methods and Multi-factor Dimensionality Reduction}
The usage of previously explained \textit{frequentist} methods, although very widely used, still pose some problems because of a limitation on the usage of \textit{p-values} themselves. From a \textit{p-value} alone it is very hard to know how confident one can be when a SNP is associated with a phenotype \cite{ioannidis2008effect}. Furthermore, the datasets utilized in these studies are usually small, an it is necessary to account for their uncertainty \cite{o2008bayesian}. By utilizing \textit{Bayesian} methods we can circumvent these issues, at the expense of additional assumptions about the influence on phenotype for each SNP. Another great advantage of these methods is that they allow for a common-ground when comparing results between studies (\textit{meta-analysis}), facilitating knowledge integration \cite{stephens2009bayesian}. It turns the probability that a SNP affects a phenotype in a quantitative measure, the Bayes Factor \textcolor{red}{Eq stephens page 4}. For these reasons, \textit{Bayesian} methods have become more prevalent in recent years in GWAS \cite{fernando2017application}. It is also readily usable in several packages, such as SNPTEST \cite{marchini2007new}, genMOSS \cite{friedlander2016analyzing} and BIMBAM \cite{guan2008practical}.\\
However, even with such techniques, the missing heritability is still yet uncovered for complex diseases, which leads us to Epistasis or gene-gene interaction, and how to integrate it in GWAS.\textcolor{red}{missing cite} When accounting for gene-gene interactions, the problem becomes statistically and computationally complex, since a \textit{k} way interaction between \textit{n} SNPs is on the $n^k$ order, with high dimensionality.\textcolor{red}{ missing cite} To attenuate these issues, and combine variants information considering gene-gene interaction, Multi-factor Dimensionality Reduction (MDR) can be used. At it's core, MDR is an algorithm capable of constructing new features by pooling genotypes from multiple SNPs \cite{moore2006flexible}. Considering several multi locus genotype information and given a threshold T, a certain group of SNPs is considered high risk if their ratio of case study to control group is higher than T, or low risk if that same ratio is lower \cite{moore2010bioinformatics}. By doing so, a new one dimension vector with two different groups (High and Low risk) is constructed, which enables the usage of other techniques to process it, and produce multi locus analysis results. This approach has been widely and successfully used in other GWAS studies, and improved with entropy-based interpretation methods, the use of odds ratio, imputation, parallel implementations and much more \cite{moore2006flexible}. Furthermore, cases like the susceptibility to bladder cancer were found in highly significant interactions between SNPs using such methods that consider epistasis, with higher prediction power than smoking \cite{stern2009polymorphisms}.

\textcolor{red}{PCA. Include??}

\subsection{Data Mining and Machine Learning}
In genomics, the vast quantities of data that must be scouted before any meaningful results are achieved is daunting, and simple frequentist methods have not yet been able to fully crack the problem of complex diseases heritability \textcolor{red}{missing cite}. As we delve into gene-gene and gene-environment interaction, non-linearity in the mapping of genotype to phenotype and how we address it becomes critical, as to understand genomic variation, disease susceptibility and the role of environment in genomics. There might be a combination of SNPs that, if addressed with the proper non-linear function, can significantly translate into the respective phenotypes. While searching for them individually, they might not appear any different than the millions of other SNPs in each sample, but they might do so if combined. To this outcome, that can't be predicted by the sum of all markers, we call non-linearity \cite{moore2010bioinformatics}.\\
To produce non-linear models, it first must be discussed how to effectively reduce the amount of SNPs that a model needs to look through, or in other words, how to data mine the genome \textcolor{red}{missing cite}. A model can also be built without any preprocessing, but besides the great quantity of features for few samples, a great majority of the millions of SNPs available are considered noise and non-significant to the task at hands. There are essentially 2 types of feature selection used, which are the filtering and the wrapper approaches \textcolor{red}{missing cite}. The first one, refers to the preprocessing of data and assessment of the quality and significance for each feature, to ultimately collect a significant subset. The wrapper one utilizes a deterministic or stochastic algorithm that iteratively selects subsets of data to classify. Filtering is a faster approach, while the wrapper can be more powerful, since it doesn't discard variables with assumptions of quality. Even if these two methods are widely used, there are multiple other ways of discarding noisy SNPs, for example, the inclusion of biological knowledge \textcolor{red}{missing cite}.\\
After the preprocessing methods are carefully selected and implemented, and subsetting has been performed, machine learning can be used to classify between the desired phenotypes. The purpose of machine learning is to make computers learn how to perform certain tasks, by providing them with a set of examples, but without explicitly programming them to do so. In our problem context, each SNP is considered a feature, which means that it is an attribute from where the model can learn. The entirety of features and samples compose the training dataset. To provide the machine with the context of what to learn, a target vector is given. This vector contains information on the phenotype that requires classification \cite{nelson2013higher}. The most popular machine learning methods are Support Vector Machines, Random Forests, Na√Øve Bayes classifiers, Neural Networks and Fuzzy Sets \cite{szymczak2009machine}. Since there is a growing interest in non-linear models, Random Forests and Neural Networks are good starting points \cite{kavakiotis2017machine}.\\
Random Forests is easily interpretable and applicable in case-control studies, and is highly adaptive to data, which makes it effective when dealing with "large $p$, small $n$" problems. Besides these advantages, it also accounts for interaction between variables, making it tailored to detect epistasis \cite{chen2012random}. The unit of Random Forests methods is the decision tree, which is based on a tree of nodes where each represents a rule. A sample is guided through all the nodes/decisions, and is ultimately placed in it's final classification of case or control. All these decision trees are formed with Bootstrapping (random samples and features), and the ultimate classification is based on the set of decisions made by all trees \cite{moore2010bioinformatics}. \textcolor{red}{image from btp713, pag 4} This classifier can be used to perform SNPs selection, genotype-phenotype association, epistasis detection and risk assessment \cite{meng2009performance, hwang2017biological}. \\
Neural Networks, more specifically, Deep Learning, is a technique based on the architecture of the biological brain, that turned out to be very good at discovering non-linear patterns in high-dimensional raw data, without much human supervision \cite{mamoshina2016applications}. The unit of a Neural Network is the neuron, which is inter-linked with other neurons in multiple layers. There is always an input layer, which feeds the hidden layers, and that ultimately leads to the output layer. Deep Learning is considered to be a Neural Network with several hidden layers (more than 3). Each neuron's output is given by the weighted sum of outputs in the layers below, to which is applied a non-linear activation function. For example, the output of the j\textsuperscript{th} neuron is given by: \[f(a\textsubscript{j}) = f(\sum_{i}^{}W\textsubscript{ij}X\textsubscript{i} + b\textsubscript{i})\], where $W$ is the weight of the neuron $X$ and $b$ is bias \cite{lecun2015deep}. To perform backward propagation, the outputs are compared with the correct answer, and error derivatives are obtained. These are used to adjust the weights and improve the outputs of the network \cite{lecun2015deep, uppu2016towards}. Deep Neural Networks have produced extremely good results in the fields of image and language processing, and speech recognition. These challenges are similar in their high dimensionality and noise rates when compared to  genotype-phenotype association problems. There have already been made applications of this methods to detect SNP interactions and perform GWAS in T2D, and although these are very recent, they show very promising results \cite{uppu2016deep, fergus2018utilising, kim2017genetic}.\\
DNN's have many benefits, but they also come with some drawbacks. For now, they lack an adequate formulation, and are considered black boxes, which makes it hard to interpret them. When uncovering associations in GWAS it is important to not lose track of the biological context of the problem, and that can be very hard when dealing with these methods \cite{mamoshina2016applications}.


\textcolor{red}{Evolutionary Computation. Include??}


\subsection{Control and Validation}
When performing a GWAS, the ultimate goal is for it to be able to predict, in any new given dataset, the interaction of markers and phenotypes that were discovered. To be able to validate this work, it is extremely important to test the results in data sets that were not used during the study, since most significant effects uncovered are likely to be overestimations \cite{xu2003theoretical}. In GWAS, there are far more SNPs than number of samples, which can easily lead to a model that predicts all cases in the discovery dataset and that cannot be replicated in others. This is called over-fitting \cite{hayes2001prediction}. Furthermore, validation leads to a higher confidence level in the study performed, and also allows to uncover the populations where it can be replicated, if at all. Validation must be first thought out when designing the study, by assigning a percentage of samples to perform testing and other for cross-validation (apart from training).\\
As most of the studies samples are from a single population, the choice of control group must also take that into consideration as a risk variant might not be relevant across all different populations. Most control groups can be gathered from the study itself, but to promote less bias, samples from the 1000 Genome Project can be used \cite{10002012integrated}. This project gathered variants from populations across the world, that serve not only as control, but as gold standards for imputation.\\

\subsection{Important points for Intro or Others \textcolor{red}{Not to include on SOTA}}
Machine that genotyped SNPs\\
Data Info\\
Divide data by sex? \\
Compare algorithms \\
